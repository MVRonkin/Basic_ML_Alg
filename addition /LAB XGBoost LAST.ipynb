{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XGBoost - модифицированный алгоритм градиентного бустинга\n",
    "\n",
    "В основе алгоритма XGBoost лежит метод градиентного бустинга на деревьях решений. \n",
    "\n",
    "Принцип градиентного бустинга — это метод машинного обучения для задач классификации и регрессии, в котором  модель предсказаний строится в форме ансамбля т.н. слабых предсказывающих моделей, обученных последовательно. Как правило в качестве слабых моделей выбирают дерево решений. Среди всех моделей самая первая (назовем ее нулевой) модель вычисляет базовое предсказание $\\hat{y}_0$. Оно, как правило, далеко не точное (поэтому каждый алгоритм слабый). Обозначим ошибку предсказаний как $\\Delta y = y - \\hat{y}_0$ На следующей итерации вычисляется вторая модель, цель которой предсказать отклонение предыдущей модели $\\Delta\\hat{y}_1$ (то есть провести корректировку модели). Таким образом ошибка модели становится $\\Delta y = y - \\left(\\hat{y}_0+\\Delta\\hat{y}_1\\right)$, в идеале ошибка должна стать меньше. На каждой следующей итерации аналогично вычисляются отклонения предсказний для результата, полученного для всех предыдущих моделей $\\Delta\\hat{y}^{(t)}$, таким образом: $\\Delta y = y - \\left(\\hat{y}_0+\\sum_{i=1}^t\\Delta\\hat{y}^{(i)}\\right)$. \n",
    "Таким образом, добавив предсказания нового слабого метода к предсказаниям уже обученного ансамбля позволяет уменьшить среднее отклонение модели. Новые оценщики добавляются в ансамбль до тех пор, пока уменьшение ошибки не остановится, либо пока не выполняется одно из правил \"ранней остановки\".\n",
    "\n",
    "Допустим, что мы вычисляем ошибку предсказаний как $\\Delta y = l(y,  \\hat{y}^0)$ для первого дерева и $\\Delta y = l(y,  \\hat{y}^{(t)})$ для каждого $t$ дерева. Тогда для выборки, состоящей из $N$ точек можно записать результат функции потерь как:\n",
    "$$L^{(t)} = \\sum_{i=1}^N l\\left(y_i, \\hat{y_i}^{(t-1)}+f_t\\right),$$\n",
    "    где:<ul>             \n",
    "        <li> $f_t = \\Delta\\hat{y}_i^{(t)}$ - результат оценки на текущем шаге - то есть оптимизируемый слабый алгоритм, например дерево решений, для которого мы ищим глубины или другие параметры;\n",
    "        <li> $\\hat{y_i}^{(t-1)}$ - результат оценки на предыдущих шагах;\n",
    "        </ul>  \n",
    "<!-- Допустим, что результат оценки на каждом текущем шаге можно записать как $\\Delta\\hat{y}_i^{(t)} = w f_t,$\n",
    "где $w$ - это некоторый весовой множитель, а $f^{(t)}$ - это непосредственный результат работы слабого алгоритма.  -->\n",
    "Пставленная задача может быть дополнена регуляризацией с целью понижения числа слабых алгоритмов и их параметров. Для деревьев решений, этим параметром может стать глубина.\n",
    "$$L^{(t)} = \\sum_{i=1}^N l\\left(y_i, \\hat{y_i}^{(t-1)}+ f_t\\right)+\\Omega(f_t),$$\n",
    "    где $\\Omega(f)$ регуляризация функции. \n",
    "    $$\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\lVert f \\rVert ^2,$$ \n",
    " где $T$ - количество листов в дереве (косвено и  глубина), а $\\gamma$ и $\\lambda$ - гиперпараметры регуляризации; $\\lVert f \\rVert ^2 = \\sum_{j=1}^T f_{jt}^2$. \n",
    "Таким образом будем минимизровать ошибку полученную в результате невязки текущего предсказания $l$, при требовании минимальности числа листов и минимальности предсказания значений самой ошибки на текущем шаге.\n",
    "\n",
    "В общем случае, в алгоритме XGBoost принято получать решение записанного выражения путем его разложения в ряд Тейлора до второго члена:\n",
    "$$L^{(t)} = \\sum_{i=1}^N l\\left(y_i,\\hat{y_i}^{(t-1)} + g_i f_t + \\frac{1}{2} h_i f_t^2\\right) + \\Omega(f_t),$$ \n",
    "где\n",
    "$$g_i = \\frac{\\partial{l(y_i,\\hat{y_i}^{t-1})}}{\\partial{\\hat{y_i}^{t-1}}}, \\quad h_i = \\frac {\\partial^2 {l(y_i,\\hat{y_i}^{t-1})}}{\\partial^2{\\hat{y_i}^{t-1}}}.$$\n",
    "Так, например, \n",
    "$$l = (y-\\hat y)^2, g = 2(y - \\hat y); h = 2.$$\n",
    "$$l = y\\ln\\left(p\\right)+\\left(1-y\\right)\\ln\\left(1-p\\right); p = \\frac{1}{1+e^{-x}}; $$\n",
    "$$g = \\frac{\\partial l}{\\partial x}=\\frac{\\partial l}{\\partial p}\\frac{\\partial p}{\\partial x}=\\frac{p-y}{\\left(1-p\\right)p}p(1-p) = p-y; \\ \n",
    "h = \\frac{\\partial^2 l}{\\partial x^2}=\\frac{\\partial p}{\\partial x}=p(1-p).$$\n",
    "\n",
    "Задача построения беггинга деревьев - минимизация функционала $L$ для каждого шага $t$. В поставленной формулировке задача может быть решена методом Ньютона. \n",
    "<!-- https://neerc.ifmo.ru/wiki/index.php?title=XGBoost -->\n",
    "\n",
    "Запишим результат функции потерь без учета постоянных слогаемых, которые не будут влиять на результат\n",
    "$$L^{(t)} \\approx \\sum_{i=1}^n [g_i f_t + \\frac{1}{2} h_i f_t^2] + \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T f_{jt}^2 $$\n",
    "Тогда можно периписать результат функции потерь для каждого листа как\n",
    "$$L^{(t)}= \\sum^T_{j=1} \\left((\\sum_{i\\in I_j} g_i) f_{jt} + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda) f_{jt}^2 \\right) + \\gamma T$$\n",
    "где $I_j$ - набор семплов для каждого листа при обучении дерева.\n",
    "\n",
    "Перепишим предыдущий результат как \n",
    "$$L^{(t)} = \\sum^T_{j=1} \\left(G_jf_{jt} + \\frac{1}{2} (H_j+\\lambda) f_{jt}^2\\right) +\\gamma T,$$\n",
    "где\n",
    "$G_j = \\sum_{i\\in I_j} g_i$ и $H_j = \\sum_{i\\in I_j} h_i$.\n",
    "\n",
    "Оптимальное значение данного уравнения (путем приравнивания проивзодной к нулю) может быть получено как\n",
    "$$f_{jt}^\\ast = -\\frac{G_j}{H_j+\\lambda}$$\n",
    "тогда\n",
    "$$L^{(t)\\ast} = -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j+\\lambda} + \\gamma T$$\n",
    "Таким образом можно переформулировать результат оптимизации градиентного бусинга XGB для каждого нового дерева решений как некоторый вес $f_{jt}$ с которым мы должны взять результат оценки получившегося листа $j$ данного дерева $t$ и добавить этот вес к результатам оценок предыдущих деревьев для формирования итогового предсказания.\n",
    "\n",
    "Заметим также, что полученный критерий может быть использован также при построении самого дерева принятия решений. То есть в ходе построения дерева можно искать то, которое бы максимально оптимизировало процедуру бустинга. \n",
    "При этом критерий максимизации расщепления можно записать как\n",
    "$$G = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma $$\n",
    "\n",
    "Теперь попробуем реализовать данную структуру\n",
    "\n",
    "\n",
    "\n",
    "<!-- readthedocs xbg doc, paper 2016-->\n",
    "\n",
    "<!-- https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32074 entries, 0 to 32073\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   Make          32074 non-null  int8 \n",
      " 1   Model         32074 non-null  int16\n",
      " 2   Style         32074 non-null  int8 \n",
      " 3   Fuel_type     32074 non-null  int8 \n",
      " 4   Transmission  32074 non-null  int8 \n",
      "dtypes: int16(1), int8(4)\n",
      "memory usage: 188.1 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"car_cat.csv\")\n",
    "df = df.drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "name_colums = df.columns.values\n",
    "df.tail()\n",
    "\n",
    "df = df.astype('category')\n",
    "\n",
    "for _, column_name in enumerate(name_colums):\n",
    "    df[column_name] =  df[column_name].cat.codes\n",
    "    \n",
    "df.info()\n",
    "\n",
    "X,y = df.drop(columns = ['Transmission']).values,df['Transmission'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим изначальную выборку ответов, которую будем считать первым предсказанием \n",
    "<!-- https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.ones_like(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем создать класс для расчета функции потерь и ее первой и второй производных\n",
    "<!-- https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLossWithLogits:\n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def activation(self,x):\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def loss(self,y, y_hat):\n",
    "        y_pred = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "        p = self.sigmoid(y_hat)\n",
    "        return y * np.log(p) + (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def gradient(self,y, y_hat):\n",
    "        p = self.sigmoid(y_hat)\n",
    "        return -(y - p)\n",
    "\n",
    "    def hessian(self,y, y_hat):\n",
    "        p = self.sigmoid(y_hat)\n",
    "        return p * (1 - p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "также перепишим функцию расчета информативности  *_gain* =\n",
    "$G^2/(H+\\lambda) $\n",
    "и расщепления \n",
    "$$G = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gain(y,y_hat,lambda_, loss_func = None):\n",
    "    if loss_func is None:\n",
    "        loss_func = LogLossWithLogits()\n",
    "    G = np.sum(loss_func.gradient(y, y_hat))\n",
    "    H = np.sum(loss_func.hessian(y, y_hat))\n",
    "    return G**2/(H+lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12186.00091936532"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gain(y,y_hat,lambda_ = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, feature, threshold):\n",
    "    idx_r = np.where(X[:,feature] <  threshold)[0]\n",
    "    idx_l = np.where(X[:,feature] >= threshold)[0]\n",
    "    return X[idx_l,:], X[idx_r,:]\n",
    "\n",
    "def split_xy(X,y,y_hat,feature, threshold):\n",
    "    Xy = np.column_stack((X,y,y_hat))\n",
    "    Xy_l, Xy_r = split(Xy, feature, threshold)\n",
    "    y_l     = Xy_l[:,-2]\n",
    "    y_r     = Xy_r[:,-2]\n",
    "    y_hat_l = Xy_l[:,-1]\n",
    "    y_hat_r = Xy_r[:,-1]    \n",
    "    return Xy_l[:,:-2], y_l, y_hat_l, Xy_r[:,:-2], y_r, y_hat_r\n",
    "\n",
    "def split_y(X,y,y_hat,feature, threshold):\n",
    "    Xy = np.column_stack((X,y,y_hat))\n",
    "    Xy_l, Xy_r = split(Xy, feature, threshold)\n",
    "    y_l     = Xy_l[:,-2]\n",
    "    y_r     = Xy_r[:,-2]\n",
    "    y_hat_l = Xy_l[:,-1]\n",
    "    y_hat_r = Xy_r[:,-1]    \n",
    "    return y_l, y_hat_l, y_r, y_hat_r\n",
    "\n",
    "def _info_gain(y,y_hat, y_l, y_hat_l, y_r, y_hat_r, lambda_, gamma, loss_func = None):\n",
    "    l   = _gain(y,  y_hat,  lambda_,loss_func)\n",
    "    l_l = _gain(y_l,y_hat_l,lambda_,loss_func)\n",
    "    l_r = _gain(y_r,y_hat_r,lambda_,loss_func)\n",
    "    return 0.5*(l_r+l_l - l) + gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат расщипления для столбца тип топлива и выбирем порог расщипления 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334.9330805570988"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_l,y_hat_l,y_r,y_hat_r = split_y(X,y,y_hat,feature=3, threshold=5)\n",
    "_info_gain(y,y_hat, y_l, y_hat_l, y_r, y_hat_r, lambda_=0.7, gamma=0.3, loss_func = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем порог для лучшего расщипления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5], dtype=int16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.160369271707168\n",
      "103.73885401959433\n",
      "98.15778417871297\n",
      "62.45943366569736\n",
      "6.491911523716226\n",
      "334.63308055709876\n"
     ]
    }
   ],
   "source": [
    "loss_func = LogLossWithLogits()\n",
    "\n",
    "for threshold in np.unique(X[:,3]):\n",
    "    y_l,y_hat_l,y_r,y_hat_r = split_y(X,y,y_hat,feature=3, threshold=threshold)\n",
    "    g = _info_gain(y,y_hat, y_l, y_hat_l, y_r, y_hat_r, lambda_=0.7, gamma=0.0, loss_func = loss_func)\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как и следовало ожидать результат остался таким же как и был в предыдущем дереве. Теперь перепишем функцию поиска наилучшего расщепления"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_threshold(X,y,y_hat, feature, lambda_, gamma, loss_func):\n",
    "    \n",
    "    thresholds = np.unique(X[:,feature])\n",
    "    gains      = np.zeros(thresholds.size)\n",
    "\n",
    "    for i,threshold in enumerate(thresholds):                    \n",
    "        y1,y_hat1, y2, y_hat2 = split_y(X,y,y_hat, feature, threshold)                    \n",
    "        \n",
    "        if (y1.size>0) and (y2.size>0):\n",
    "            gains[i] = _info_gain(y,y_hat, \n",
    "                                  y1, y_hat1, \n",
    "                                  y2, y_hat2, \n",
    "                                  lambda_, \n",
    "                                  gamma, \n",
    "                                  loss_func)\n",
    "        else:\n",
    "             gains[i] = 0\n",
    "    \n",
    "    idx_max = np.argmax(gains)            \n",
    "    return thresholds[idx_max], gains[idx_max]\n",
    "\n",
    "def best_split(X, y, y_hat, lambda_, gamma, loss_func):\n",
    "    samples, features = X.shape\n",
    "    gains      = np.zeros(features)\n",
    "    thresholds = np.zeros(features) \n",
    "    \n",
    "    for feature in range(features):\n",
    "        thresholds[feature], gains[feature] = best_threshold(X,y,y_hat, feature, lambda_, gamma, loss_func)\n",
    "\n",
    "    best_feature = np.argmax(gains)  # best index    \n",
    "    \n",
    "    return best_feature,thresholds[best_feature], gains[best_feature] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 35.0, 1043.239899388879)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lambda_ = 0.7\n",
    "gamma   = 0.3\n",
    "loss_func = LogLossWithLogits()\n",
    "best_split(X, y, y_hat, lambda_, gamma, loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запишем базовый класс XGB дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicXGBTree(BasicTree):\n",
    "    def __init__(self, \n",
    "                 min_samples_split=2,\n",
    "                 min_samples_leaf = 1,\n",
    "                 min_gain=1e-7,\n",
    "                 max_depth=-1,\n",
    "                 lambda_=0.0, \n",
    "                 gamma=0.0, \n",
    "                 loss_func = None):\n",
    "        \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma   = gamma\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        if self.loss_func is None:\n",
    "            self.loss_func = LogLossWithLogits()\n",
    "        \n",
    "        super(BasicXGBTree,self).__init__(min_samples_split,\n",
    "                                          min_samples_leaf,\n",
    "                                          min_gain,\n",
    "                                          max_depth)\n",
    "        \n",
    "    #-----------------------------------------------\n",
    "    # Class method \n",
    "    def fit(self, X, y, y_hat):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.root = Node(depth = 1)\n",
    "        self.root = self._build(X, y, y_hat)\n",
    "        return self\n",
    "    \n",
    "    #-----------------------------------------------\n",
    "    # Class method \n",
    "    def _build(self, X, y, y_hat, depth=1):\n",
    "        max_gain = 1\n",
    "        best_feature   = 0\n",
    "        best_threshold = 0\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and depth <= self.max_depth:\n",
    "\n",
    "            best_feature, best_threshold, max_gain= self._best_split(X,y, y_hat)\n",
    "            \n",
    "            if max_gain > self.min_gain:\n",
    "\n",
    "                X_l,y_l,y_hat_l,X_r,y_r,y_hat_r = self._split_xy(X,y, y_hat, best_feature, best_threshold)\n",
    "                \n",
    "                return Node(feature=best_feature, \n",
    "                            threshold=best_threshold, \n",
    "                            left    = self._build(X_l,y_l, y_hat_l, depth + 1), #true_branch, \n",
    "                            right   = self._build(X_r,y_r, y_hat_r, depth + 1), #false_branch\n",
    "                            depth   = depth,\n",
    "                            samples = y.size,\n",
    "                            gain    = max_gain,\n",
    "                            probability = self._probabilities(y))\n",
    "\n",
    "        \n",
    "        leaf_value = self._leaf_weight(y, y_hat) \n",
    "        p = self._probabilities(y)\n",
    "        return Node(value   = leaf_value, \n",
    "                    is_leaf = True, \n",
    "                    depth   = depth,\n",
    "                    samples = y.size,\n",
    "                    gain    = max_gain,\n",
    "                    probability = p)\n",
    "\n",
    "    #------------------------------------------\n",
    "    # Generic method\n",
    "    def _best_split(self,X, y, y_hat):\n",
    "        best_feature, best_threshold, max_gain = 0,0,1\n",
    "        return best_feature, best_threshold, max_gain\n",
    "\n",
    "    #------------------------------------------\n",
    "    def _leaf_weight(self, y, y_hat):\n",
    "        return self._to_result(y)   \n",
    "    #------------------------------------------\n",
    "    def _split(self, X, feature, threshold):\n",
    "        idx_r = np.where(X[:,feature] <  threshold)[0]\n",
    "        idx_l = np.where(X[:,feature] >= threshold)[0]\n",
    "        return X[idx_l,:], X[idx_r,:]\n",
    "    \n",
    "    #------------------------------------------ \n",
    "    def _split_xy(self,X,y,y_hat,feature, threshold):\n",
    "        Xy = np.column_stack((X,y,y_hat))\n",
    "        Xy_l, Xy_r = self._split(Xy, feature, threshold)\n",
    "        y_l     = Xy_l[:,-2]\n",
    "        y_r     = Xy_r[:,-2]\n",
    "        y_hat_l = Xy_l[:,-1]\n",
    "        y_hat_r = Xy_r[:,-1]    \n",
    "        return Xy_l[:,:-2], y_l, y_hat_l, Xy_r[:,:-2], y_r, y_hat_r\n",
    "    #------------------------------------------\n",
    "    def _split_y(self,X,y,y_hat,feature, threshold):\n",
    "        Xy = np.column_stack((X,y,y_hat))\n",
    "        Xy_l, Xy_r = self._split(Xy, feature, threshold)\n",
    "        y_l     = Xy_l[:,-2]\n",
    "        y_r     = Xy_r[:,-2]\n",
    "        y_hat_l = Xy_l[:,-1]\n",
    "        y_hat_r = Xy_r[:,-1]    \n",
    "        return y_l, y_hat_l, y_r, y_hat_r\n",
    "    \n",
    "    #------------------------------------------\n",
    "    def _predict(self, x, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        \n",
    "        if tree.is_leaf:\n",
    "            return tree.value\n",
    "\n",
    "        if x[tree.feature] >= tree.threshold:\n",
    "            branch = tree.left\n",
    "        else:\n",
    "            branch = tree.right\n",
    "\n",
    "        return self._predict(x, branch) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверка работоспособности дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.000?\n",
      "   BRANCH = 1_LEFT (32074 samples)-> 0:0.000?\n",
      "      BRANCH = 2_LEFT (32074 samples)-> 0:0.000?\n",
      "            BRANCH = 3_LEFT (32074 samples)-> 0 [0.54233959 0.45766041]\n",
      "            BRANCH = 3_RIGHT (32074 samples)-> 0 []\n",
      "      BRANCH = 2_RIGHT (32074 samples)-> 0 []\n",
      "   BRANCH = 1_RIGHT (32074 samples)-> 0 []\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.zeros_like(y)\n",
    "tree = BasicXGBTree(max_depth=3)\n",
    "tree.fit(X,y, y_hat)\n",
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим одно тестовое XGB дерево для классификации. Его отличием от реальных деревьев будет то, что мы не будем присваивать полученным листам весовые коэффициенты. В данном дереве мы проверим работоспособность критерия расщипления, записанного выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestClassificationXGBTRee(BasicXGBTree):\n",
    "    #------------------------------------------ \n",
    "    def _best_threshold(self,X,y, y_hat, feature):\n",
    "\n",
    "        thresholds = np.unique(X[:,feature])\n",
    "        gains      = np.zeros(thresholds.size)\n",
    "\n",
    "        for i,threshold in enumerate(thresholds):                    \n",
    "            y1,y_hat1, y2, y_hat2 = self._split_y(X,y,y_hat,feature,threshold)                    \n",
    "\n",
    "            if (y1.size>0) and (y2.size>0):\n",
    "                gains[i] = self._info_gain(y,y_hat, \n",
    "                                           y1, y_hat1, \n",
    "                                           y2, y_hat2)\n",
    "            else:\n",
    "                 gains[i] = 0\n",
    "\n",
    "        idx_max = np.argmax(gains)            \n",
    "        return thresholds[idx_max], gains[idx_max]\n",
    "\n",
    "    #------------------------------------------ \n",
    "    def _best_split(self,X, y, y_hat):\n",
    "        samples, features = X.shape\n",
    "        gains      = np.zeros(features)\n",
    "        thresholds = np.zeros(features) \n",
    "\n",
    "        for feature in range(features):\n",
    "            thresholds[feature], gains[feature] = self._best_threshold(X,y,y_hat, feature)\n",
    "\n",
    "        best_feature = np.argmax(gains)  # best index    \n",
    "\n",
    "        return best_feature,thresholds[best_feature], gains[best_feature]    \n",
    "    \n",
    "    #------------------------------------------ \n",
    "    def _info_gain(self,y,y_hat, y_l, y_hat_l, y_r, y_hat_r):\n",
    "        l = self._loss(y,y_hat)\n",
    "        l_l = self._loss(y_l,y_hat_l)\n",
    "        l_r = self._loss(y_r,y_hat_r)\n",
    "        return 0.5*(l_r+l_l - l) + self.gamma\n",
    "    \n",
    "    #------------------------------------------ \n",
    "    def _loss(self,y,y_hat):\n",
    "        G = np.sum(self.loss_func.gradient(y, y_hat))\n",
    "        H = np.sum(self.loss_func.hessian(y, y_hat))\n",
    "        return G**2/(H+self.lambda_)\n",
    "    \n",
    "    #------------------------------------------\n",
    "    def _to_result(self, y):\n",
    "        return self._to_class(y)\n",
    "    \n",
    "    #------------------------------------------\n",
    "    def _to_class(self, y):\n",
    "        y = np.asarray(y)\n",
    "        if y.ndim >0:\n",
    "            values,counts = np.unique(y, return_counts=True)\n",
    "            i_max = np.argmax(counts)\n",
    "            return values[i_max]\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    #---------------------------------\n",
    "    def score(self, X, y):\n",
    "        yhat  = self.predict(X)\n",
    "        return sum((yhat==y)*1)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat = np.zeros_like(y_train)\n",
    "tree = TestClassificationXGBTRee(min_samples_split=10,\n",
    "                                 min_gain=1e-02,\n",
    "                                 max_depth=5,\n",
    "                                 lambda_=0.5, \n",
    "                                 gamma=0.3).fit(X_train,y_train,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7209809830614153 0.7156474099149258\n"
     ]
    }
   ],
   "source": [
    "print(tree.score(X_test,y_test),  tree.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:35.000?\n",
      "   BRANCH = 1_LEFT (22451 samples)-> 0:39.000?\n",
      "      BRANCH = 2_LEFT (11208 samples)-> 2:4.000?\n",
      "            BRANCH = 3_LEFT (8774 samples)-> 1:113.000?\n",
      "                        BRANCH = 4_LEFT (7802 samples)-> 2:8.000?\n",
      "                                                BRANCH = 5_LEFT (1024 samples)-> 1 [0.34303216 0.65696784]\n",
      "                                                BRANCH = 5_RIGHT (1024 samples)-> 0 [0.65229111 0.34770889]\n",
      "                        BRANCH = 4_RIGHT (7802 samples)-> 3:1.000?\n",
      "                                                BRANCH = 5_LEFT (6778 samples)-> 0 [0.57292528 0.42707472]\n",
      "                                                BRANCH = 5_RIGHT (6778 samples)-> 0 [0.76261504 0.23738496]\n",
      "            BRANCH = 3_RIGHT (8774 samples)-> 2:3.000?\n",
      "                        BRANCH = 4_LEFT (972 samples)-> 1:80.000?\n",
      "                                                BRANCH = 5_LEFT (730 samples)-> 1 [0.27586207 0.72413793]\n",
      "                                                BRANCH = 5_RIGHT (730 samples)-> 1 [0.092827 0.907173]\n",
      "                        BRANCH = 4_RIGHT (972 samples)-> 1:88.000?\n",
      "                                                BRANCH = 5_LEFT (242 samples)-> 0 [0.55445545 0.44554455]\n",
      "                                                BRANCH = 5_RIGHT (242 samples)-> 0 [0.87943262 0.12056738]\n",
      "      BRANCH = 2_RIGHT (11208 samples)-> 1:46.000?\n",
      "            BRANCH = 3_LEFT (2434 samples)-> 3:4.000?\n",
      "                        BRANCH = 4_LEFT (1695 samples)-> 2:4.000?\n",
      "                                                BRANCH = 5_LEFT (613 samples)-> 0 [0.78732106 0.21267894]\n",
      "                                                BRANCH = 5_RIGHT (613 samples)-> 1 [0.39516129 0.60483871]\n",
      "                        BRANCH = 4_RIGHT (1695 samples)-> 1:101.000?\n",
      "                                                BRANCH = 5_LEFT (1082 samples)-> 0 [0.7601476 0.2398524]\n",
      "                                                BRANCH = 5_RIGHT (1082 samples)-> 0 [0.90925926 0.09074074]\n",
      "            BRANCH = 3_RIGHT (2434 samples)-> 3:4.000?\n",
      "                        BRANCH = 4_LEFT (739 samples)-> 0:37.000?\n",
      "                                                BRANCH = 5_LEFT (286 samples)-> 0 [0.85140562 0.14859438]\n",
      "                                                BRANCH = 5_RIGHT (286 samples)-> 0 [1.]\n",
      "                        BRANCH = 4_RIGHT (739 samples)-> 2:5.000?\n",
      "                                                BRANCH = 5_LEFT (453 samples)-> 0 [0.98214286 0.01785714]\n",
      "                                                BRANCH = 5_RIGHT (453 samples)-> 0 [0.88034188 0.11965812]\n",
      "   BRANCH = 1_RIGHT (22451 samples)-> 0:21.000?\n",
      "      BRANCH = 2_LEFT (11243 samples)-> 1:14.000?\n",
      "            BRANCH = 3_LEFT (5017 samples)-> 1:124.000?\n",
      "                        BRANCH = 4_LEFT (4822 samples)-> 0:32.000?\n",
      "                                                BRANCH = 5_LEFT (252 samples)-> 0 [0.75252525 0.24747475]\n",
      "                                                BRANCH = 5_RIGHT (252 samples)-> 1 [0.40740741 0.59259259]\n",
      "                        BRANCH = 4_RIGHT (4822 samples)-> 3:3.000?\n",
      "                                                BRANCH = 5_LEFT (4570 samples)-> 1 [0.36970603 0.63029397]\n",
      "                                                BRANCH = 5_RIGHT (4570 samples)-> 1 [0.21342177 0.78657823]\n",
      "            BRANCH = 3_RIGHT (5017 samples)-> 1:8.000?\n",
      "                        BRANCH = 4_LEFT (195 samples)-> 2:10.000?\n",
      "                                                BRANCH = 5_LEFT (160 samples)-> 0 [0.77227723 0.22772277]\n",
      "                                                BRANCH = 5_RIGHT (160 samples)-> 0 [0.94915254 0.05084746]\n",
      "                        BRANCH = 4_RIGHT (195 samples)-> 2:10.000?\n",
      "                                                BRANCH = 5_LEFT (35 samples)-> 0 [0.52380952 0.47619048]\n",
      "                                                BRANCH = 5_RIGHT (35 samples)-> 0 [0.71428571 0.28571429]\n",
      "      BRANCH = 2_RIGHT (11243 samples)-> 0:11.000?\n",
      "            BRANCH = 3_LEFT (6226 samples)-> 0:14.000?\n",
      "                        BRANCH = 4_LEFT (2959 samples)-> 3:1.000?\n",
      "                                                BRANCH = 5_LEFT (1791 samples)-> 1 [0.44278169 0.55721831]\n",
      "                                                BRANCH = 5_RIGHT (1791 samples)-> 0 [0.77709924 0.22290076]\n",
      "                        BRANCH = 4_RIGHT (2959 samples)-> 0:12.000?\n",
      "                                                BRANCH = 5_LEFT (1168 samples)-> 0 [0.97210744 0.02789256]\n",
      "                                                BRANCH = 5_RIGHT (1168 samples)-> 0 [0.745 0.255]\n",
      "            BRANCH = 3_RIGHT (6226 samples)-> 1:93.000?\n",
      "                        BRANCH = 4_LEFT (3267 samples)-> 0:5.000?\n",
      "                                                BRANCH = 5_LEFT (1232 samples)-> 1 [0.42379182 0.57620818]\n",
      "                                                BRANCH = 5_RIGHT (1232 samples)-> 1 [0.12253375 0.87746625]\n",
      "                        BRANCH = 4_RIGHT (3267 samples)-> 1:9.000?\n",
      "                                                BRANCH = 5_LEFT (2035 samples)-> 1 [0.38690476 0.61309524]\n",
      "                                                BRANCH = 5_RIGHT (2035 samples)-> 0 [0.67323944 0.32676056]\n"
     ]
    }
   ],
   "source": [
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Теперь допустим, что мы будем строить ансамбль деревьев. \n",
    "\n",
    "Как уже отмечалось для каждого дерева в ансамбле мы будем вычислять взвешенное предсказание $f_t^\\ast$.\n",
    "\n",
    "Также отметим, что для получения итогового предсказания класса мы будем использовать функцию активации сигмоид (мы учитывали ее в функции потерь) и значение порога, по которому мы будем принимать решение о классе.\n",
    "\n",
    "Также в дерево добавлена информация о предыдущих деревьях, что необходимо для расчета итогового предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationXGBTRee(TestClassificationXGBTRee):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 min_samples_split = 2,\n",
    "                 min_samples_leaf  = 1,\n",
    "                 min_gain          = 1e-7,\n",
    "                 max_depth         = -1,\n",
    "                 lambda_           = 0.0, \n",
    "                 gamma             = 0.0, \n",
    "                 class_threshold   = 0.5,\n",
    "                 loss_func         = None):\n",
    "        \n",
    "        super(ClassificationXGBTRee,self).__init__(min_samples_split,\n",
    "                                                   min_samples_leaf,\n",
    "                                                   min_gain,\n",
    "                                                   max_depth,\n",
    "                                                   lambda_, \n",
    "                                                   gamma, \n",
    "                                                   loss_func)\n",
    "        self.class_threshold = class_threshold\n",
    "    \n",
    "    #------------------------------------------\n",
    "    def _leaf_weight(self, y, y_hat):\n",
    "        G = np.sum(self.loss_func.gradient(y, y_hat))\n",
    "        H = np.sum(self.loss_func.hessian(y, y_hat))\n",
    "        return -G/(H+self.lambda_)\n",
    "\n",
    "    #------------------------------------------\n",
    "    def get_weights(self, x, y_previous = None):\n",
    "        y_f = np.asarray([self._predict(sample) for sample in x])\n",
    "        if y_previous is None:\n",
    "            return y_f\n",
    "        else:\n",
    "            return y_f + y_previous\n",
    "        \n",
    "    #------------------------------------------\n",
    "    def predict(self, x, y_previous = None): \n",
    "        return self.loss_func.activation(self.get_weights(x, y_previous))>= self.class_threshold\n",
    "    \n",
    "    #---------------------------------\n",
    "    def score(self, X, y, y_previous = None):\n",
    "        yhat  = self.predict(X, y_previous)\n",
    "        return sum((yhat==y)*1)/y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим первое дерево решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_hat = np.zeros_like(y_train)\n",
    "\n",
    "tree = ClassificationXGBTRee(min_samples_split=10,\n",
    "                                 min_gain=1e-02,\n",
    "                                 max_depth=5,\n",
    "                                 lambda_=0.5, \n",
    "                                 gamma=0.3).fit(X_train,y_train,y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7209809830614153 0.7156474099149258\n"
     ]
    }
   ],
   "source": [
    "print(tree.score(X_test,y_test),  tree.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим значения весовых параметров для тренировочного и тестового наборов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = tree.get_weights(X_train)\n",
    "y_hat_test  = tree.get_weights(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь создадим второе дерево"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2 = ClassificationXGBTRee(min_samples_split=10,\n",
    "                                 min_gain=1e-02,\n",
    "                                 max_depth=5,\n",
    "                                 lambda_=0.5, \n",
    "                                 gamma=0.3).fit(X_train,y_train,y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767743946794139 0.7625495523584696\n"
     ]
    }
   ],
   "source": [
    "print(tree2.score(X_test,y_test,y_hat_test),  tree2.score(X_train,y_train,y_hat_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что точность стала выше. Попробуем еще раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = tree2.get_weights(X_train,y_hat_train)\n",
    "y_hat_test  = tree2.get_weights(X_test,y_hat_test)\n",
    "\n",
    "tree3 = ClassificationXGBTRee(min_samples_split=10,\n",
    "                                 min_gain=1e-02,\n",
    "                                 max_depth=5,\n",
    "                                 lambda_=0.5, \n",
    "                                 gamma=0.3).fit(X_train,y_train,y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7743946794139042 0.7671373212774487\n"
     ]
    }
   ],
   "source": [
    "print(tree3.score(X_test,y_test,y_hat_test),  tree3.score(X_train,y_train,y_hat_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс для работы с XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBClassifier:\n",
    "    def __init__(self,\n",
    "                 max_estimators    = 1,\n",
    "                 min_samples_split = 2,\n",
    "                 min_samples_leaf  = 1,\n",
    "                 min_gain          = 1e-7,\n",
    "                 max_depth         = -1,\n",
    "                 lambda_           = 0.0, \n",
    "                 gamma             = 0.0, \n",
    "                 class_threshold   = 0.5,\n",
    "                 min_score         = 1e-5,\n",
    "                 loss_func         = None):\n",
    "        \n",
    "        self.max_estimators    = max_estimators\n",
    "        self.min_samples_split = min_samples_split       \n",
    "        self.min_samples_leaf  = min_samples_leaf     \n",
    "        self.min_gain          = min_gain      \n",
    "        self.max_depth         = max_depth      \n",
    "        self.lambda_           = lambda_\n",
    "        self.gamma             = gamma\n",
    "        self.class_threshold   = class_threshold\n",
    "        \n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        if self.loss_func is None:\n",
    "            self.loss_func = LogLossWithLogits()\n",
    "        \n",
    "        self.min_score = min_score\n",
    "        self.trees     = []\n",
    "        self.n_trees   = 0\n",
    "    \n",
    "    #----------------------------------    \n",
    "    def fit(self,X,y, X_val=None,y_val=None, verbose=True):\n",
    "        y_hat = np.zeros_like(y)\n",
    "        self.trees = np.array([])\n",
    "\n",
    "        if (X_val is None and y_val is None):\n",
    "            val_flag  = False\n",
    "        else :\n",
    "            val_flag  = True\n",
    "            y_hat_val = np.zeros_like(y_val)\n",
    "\n",
    "        \n",
    "        for i in range(self.max_estimators):\n",
    "            tree = ClassificationXGBTRee(min_samples_split = self.min_samples_split,\n",
    "                                         min_samples_leaf  = self.min_samples_leaf,\n",
    "                                         min_gain          = self.min_gain,\n",
    "                                         max_depth         = self.max_depth,\n",
    "                                         lambda_           = self.lambda_,\n",
    "                                         gamma             = self.gamma,\n",
    "                                         class_threshold   = self.class_threshold,\n",
    "                                         loss_func         = self.loss_func\n",
    "                                         ).fit(X,y,y_hat)\n",
    "            \n",
    "\n",
    "            y_hat = tree.get_weights(X,y_hat)\n",
    "            self.trees = np.append(self.trees, tree)\n",
    "\n",
    "            if verbose:\n",
    "                print('='*10)\n",
    "                if val_flag:\n",
    "                    y_hat_val = tree.get_weights(X_val,y_hat_val)\n",
    "                    print('i = ',i,'; train score = %.4f'%tree.score(X,y,y_hat),  'val score = %.4f'%tree.score(X_val,y_val,y_hat_val))\n",
    "                else:\n",
    "                    print('i = ',i,'; score = %.4f'%tree.score(X,y,y_hat))\n",
    "\n",
    "        self.n_trees = i+1\n",
    "        return self\n",
    "    #--------------------\n",
    "    def predict(self,X):\n",
    "\n",
    "        y_hat = np.zeros(X.shape[0])\n",
    "\n",
    "        for i in range(self.n_trees):\n",
    "            y_hat = self.trees[i].get_weights(X,y_hat)\n",
    "\n",
    "        return self.loss_func.activation(y_hat)>= self.class_threshold\n",
    "    \n",
    "    #--------------------\n",
    "    def score(self, X, y):\n",
    "        yhat  = self.predict(X)\n",
    "        return sum((yhat==y)*1)/y.size\n",
    "    \n",
    "    #--------------------\n",
    "    def print_scores(self, X, y):\n",
    "        y_hat = np.zeros(X.shape[0])\n",
    "\n",
    "        for i in range(self.n_trees):\n",
    "            y_hat = self.trees[i].get_weights(X,y_hat)   \n",
    "            print('i = ',i,'; train score = %.4f'%self.trees[i].score(X,y,y_hat))\n",
    "        \n",
    "        print('overall score = %.4f'%self.score(X,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "i =  0 ; score = 0.7156\n",
      "==========\n",
      "i =  1 ; score = 0.7507\n",
      "==========\n",
      "i =  2 ; score = 0.7659\n",
      "==========\n",
      "i =  3 ; score = 0.7664\n",
      "==========\n",
      "i =  4 ; score = 0.7771\n",
      "==========\n",
      "i =  5 ; score = 0.7874\n",
      "==========\n",
      "i =  6 ; score = 0.7905\n",
      "==========\n",
      "i =  7 ; score = 0.7985\n",
      "==========\n",
      "i =  8 ; score = 0.8048\n",
      "==========\n",
      "i =  9 ; score = 0.8030\n",
      "==========\n",
      "i =  10 ; score = 0.8059\n",
      "==========\n",
      "i =  11 ; score = 0.8042\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb = XGBClassifier(max_estimators    = 12,\n",
    "                    min_samples_split = 10,\n",
    "                    min_gain          = 1e-02,\n",
    "                    max_depth         = 5,\n",
    "                    lambda_           = 0.5, \n",
    "                    gamma             = 0.0\n",
    "                   ).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8064013301465239 0.8097189434769052\n"
     ]
    }
   ],
   "source": [
    "print(xgb.score(X_test,y_test),  xgb.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0 ; train score = 0.7210\n",
      "i =  1 ; train score = 0.7571\n",
      "i =  2 ; train score = 0.7735\n",
      "i =  3 ; train score = 0.7762\n",
      "i =  4 ; train score = 0.7851\n",
      "i =  5 ; train score = 0.7908\n",
      "i =  6 ; train score = 0.7923\n",
      "i =  7 ; train score = 0.7963\n",
      "i =  8 ; train score = 0.8020\n",
      "i =  9 ; train score = 0.8012\n",
      "i =  10 ; train score = 0.8030\n",
      "i =  11 ; train score = 0.8018\n",
      "overall score = 0.8064\n"
     ]
    }
   ],
   "source": [
    "xgb.print_scores(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Задание\n",
    "Добавьте к процессу обучения процедуру ранней остановки по условию прекращения роста (или падению) точности на валидационной выборке ($X_{val}, y_{val}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Задание (Необязательное)\n",
    "\n",
    "создайте XGB дерево для задачи регрессии $L_2$\n",
    "и класс XGBRegression для решения соответствующей задачи\n",
    "\n",
    "\n",
    "<!-- # https://habr.com/ru/company/vk/blog/438562/m -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
